{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ta\n",
    "import os\n",
    "import math\n",
    "import tqdm\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cufflinks as cf\n",
    "import tensorflow as tf\n",
    "import plotly.express as px\n",
    "import chart_studio.plotly as py\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "MODELS_DIR = \"Models\"\n",
    "DATA_DIR = \"Data\"\n",
    "\n",
    "\n",
    "def next_free_model_name(base_name: str) -> str:\n",
    "    model_files = [name for name in os.listdir(MODELS_DIR) if name.split(\".\")[-1].lower() == \"h5\"]\n",
    "    similar_models_numbers = [int(name.split(\"_\")[-1].split(\".\")[0]) for name in model_files if name.find(base_name) >= 0]\n",
    "    \n",
    "    if len(similar_models_numbers) > 0:\n",
    "        max_value = str(max(similar_models_numbers)+1).zfill(3)\n",
    "        new_name = base_name + \"_\" + max_value\n",
    "    else:\n",
    "        new_name = base_name + \"_000\"\n",
    "    return new_name\n",
    "\n",
    "\n",
    "def infinite_iterator(data: list or tuple or str) -> list or tuple or char or str:\n",
    "    while True:\n",
    "        for item in data:\n",
    "            yield item\n",
    "            \n",
    "\n",
    "def generate_input_sequences(data: pd.DataFrame or np.ndarray, input_length=30, output_length=1) -> np.ndarray:\n",
    "    if type(data) == pd.DataFrame:\n",
    "        price_array = data.to_numpy()\n",
    "    else:\n",
    "        price_array= data\n",
    "    input_sequences = np.zeros((price_array.shape[0] - input_length - output_length,\n",
    "                                input_length, price_array.shape[1]), dtype=float)\n",
    "    for index in range(input_sequences.shape[0]):\n",
    "        input_sequences[index] = price_array[index:index+input_length, :]\n",
    "    return input_sequences\n",
    "\n",
    "\n",
    "def generate_output_sequences(data: pd.DataFrame or np.ndarray, outputs: list,\n",
    "                              input_length=30, output_length=1) -> np.ndarray:\n",
    "    if type(data) == pd.DataFrame:\n",
    "        selected_dataframe = data[outputs]\n",
    "        price_array = selected_dataframe.to_numpy()\n",
    "    else:\n",
    "        price_array = data[:,outputs]\n",
    "        if price_array.ndim == 1:\n",
    "            price_array = price_array.reshape((-1,1))\n",
    "        \n",
    "    if output_length == 1:\n",
    "        output_sequences = np.zeros((price_array.shape[0] - input_length - output_length,\n",
    "                                     price_array.shape[1]), dtype=float)\n",
    "        for index in range(output_sequences.shape[0]):\n",
    "            output_sequences[index] = price_array[index+input_length : index+input_length+output_length, :] \n",
    "    else:\n",
    "        output_sequences = np.zeros((price_array.shape[0] - input_length - output_length,\n",
    "                                     output_length, price_array.shape[1]), dtype=float)\n",
    "        for index in range(output_sequences.shape[0]):\n",
    "            output_sequences[index] = price_array[index+input_length : index+input_length+output_length, :]\n",
    "    return output_sequences\n",
    "\n",
    "\n",
    "def scale_closing_data(data: np.ndarray, max_value: float) -> np.ndarray:\n",
    "    for company in data:\n",
    "#         company[1:,-1] = (company[1:,-1] - company[:-1,-1]) / max_value\n",
    "        company[1:,-1] = company[1:,-1] / max_value\n",
    "    return data\n",
    "\n",
    "\n",
    "def flatten_data(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Stacks mutlidimensional arrays on top of each other reducing their dimensionality by 1 (starting from the higgest dimension)\n",
    "    \"\"\"\n",
    "    new_data = None\n",
    "    for single_read in data:\n",
    "        if new_data is None:\n",
    "            new_data = single_read\n",
    "        else:\n",
    "            new_data = np.append(new_data, single_read, axis=0)   \n",
    "    return new_data\n",
    "\n",
    "\n",
    "def scale_3d_price_data(price_data: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Scale 3d tensor 3rd dimension using sklearn.preprocessing.MinMaxScaler\n",
    "    \"\"\"\n",
    "    scalers = []\n",
    "    scaled_price_data = []\n",
    "    for company in price_data:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(company)\n",
    "        scalers.append(scaler)\n",
    "        scaled_price_data.append(scaler.transform(company))\n",
    "    \n",
    "    return (np.array(scaled_price_data, dtype=object), scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-burden",
   "metadata": {},
   "source": [
    "# New York Stock Exchange Data\n",
    "Dataset seleceted for project [New York Stock Exchange Data] contains large amount of data separated into 4 files.  \n",
    "____\n",
    "* **fundamentals.csv**  \n",
    "metrics extracted from annual SEC 10K fillings (2012-2016), should be enough to derive most of popular fundamental indicators.\n",
    "----\n",
    "* **securities.csv**  \n",
    "general description of each company with division on sectors  \n",
    "----\n",
    "* **prices-split-adjusted.csv**  \n",
    "same as prices, but there have been added adjustments for splits.\n",
    "----\n",
    "* **prices.csv**  \n",
    "raw, as-is daily prices. Most of data spans from 2010 to the end 2016, for companies new on stock market date range is shorter. There have been approx. 140 stock splits in that time, this set doesn't account for that.\n",
    "----\n",
    "\n",
    "[New York Stock Exchange Data]: <https://www.kaggle.com/dgawlik/nyse?select=prices.csv>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "import pandas as pd\n",
    "\n",
    "fundamentals = pd.read_csv(DATA_DIR + \"/\" + \"fundamentals.csv\", index_col=0)\n",
    "securities   = pd.read_csv(DATA_DIR + \"/\" + \"securities.csv\")\n",
    "prices       = pd.read_csv(DATA_DIR + \"/\" + \"prices-split-adjusted.csv\")\n",
    "prices[prices[\"symbol\"] == \"MSFT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 80)\n",
    "fundamentals[fundamentals[\"Ticker Symbol\"] == \"MSFT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "securities[securities[\"Ticker symbol\"] == \"MSFT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique companies: { len(prices['symbol'].unique()) : >4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-chester",
   "metadata": {},
   "source": [
    "# Selected stock indicators\n",
    "____\n",
    "* __On-Balance Volume__ *(OBV)*  \n",
    "Indicator of stock momentum based on close price and stock volume\n",
    "____\n",
    "* __Accumulation/Distribution Index__ *(ADI)*  \n",
    "money flow indicator considers stock volume as well as closing price in regard to price range.\n",
    "____\n",
    "* __Aroon Indicator__ *(AI / AIU / AID)*  \n",
    "Trend following indicator, describes strength of the current trend and likely hood that it will continue.  \n",
    "May be divided into up and down indexes.\n",
    "____\n",
    "* __Relative Strength Index__ *(RSI)*  \n",
    "Measurement of the of price change magnitude, indicates overbought or oversold conditions in the price of a stock.\n",
    "____\n",
    "* __Volume Weighted Average Price__ *(VWAP)*\n",
    "Average price calculated in regard to stock volume.\n",
    "____\n",
    "* __Simple Moving Average 7 Days__ *(SMA7)*  \n",
    "Moving average over 7 day period\n",
    "____\n",
    "* __Simple Moving Average 14 Days__ *(SMA14)*  \n",
    "Moving average over 14 day period\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"MSFT\"\n",
    "stock_price = prices[prices[\"symbol\"] == ticker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBV = ta.volume.on_balance_volume(stock_price[\"close\"], stock_price[\"volume\"])\n",
    "fig = px.line(stock_price, x=stock_price[\"date\"], y=OBV, labels=dict(y=\"On-Balance Volume\"))\n",
    "fig.update_layout(template=\"simple_white\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = ta.volume.AccDistIndexIndicator(stock_price[\"high\"], stock_price[\"low\"],\n",
    "                                            stock_price[\"close\"], stock_price[\"volume\"])\n",
    "ADI = indicator.acc_dist_index()\n",
    "fig = px.line(stock_price, x=stock_price[\"date\"], y=ADI, labels=dict(y=\"Accumulation & Distribution Index\"))\n",
    "fig.update_layout(template=\"simple_white\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = ta.trend.AroonIndicator(stock_price[\"close\"])\n",
    "AI  = indicator.aroon_indicator()\n",
    "AIU = indicator.aroon_up()\n",
    "AID = indicator.aroon_down()\n",
    "\n",
    "AI_df = pd.concat([stock_price[\"date\"], AI], axis=1)\n",
    "AI_df[\"name\"] = \"AI\"\n",
    "AI_df = AI_df.rename(columns={AI_df.columns[0]: \"date\", AI_df.columns[1]: \"value\"})\n",
    "AIU_df = pd.concat([stock_price[\"date\"], AIU], axis=1)\n",
    "AIU_df[\"name\"] = \"AIU\"\n",
    "AIU_df = AIU_df.rename(columns={AIU_df.columns[1]: \"date\", AIU_df.columns[1]: \"value\"})\n",
    "AID_df = pd.concat([stock_price[\"date\"], AID], axis=1)\n",
    "AID_df[\"name\"] = \"AID\"\n",
    "AID_df = AID_df.rename(columns={AID_df.columns[1]: \"date\", AID_df.columns[1]: \"value\"})\n",
    "AI_df = AI_df.append([AIU_df, AID_df])\n",
    "\n",
    "fig = px.line(AI_df, x=\"date\", y=\"value\", color=\"name\")\n",
    "fig.update_layout(template=\"simple_white\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = ta.momentum.RSIIndicator(close=stock_price[\"close\"], window=14)\n",
    "RSI = indicator.rsi()\n",
    "\n",
    "fig = px.line(stock_price, x=stock_price[\"date\"], y=RSI, labels=dict(y=\"Relative Strength Index\"))\n",
    "fig.update_layout(template=\"simple_white\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-terminology",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indicator = ta.volume.VolumeWeightedAveragePrice(stock_price[\"high\"], stock_price[\"low\"], \n",
    "                                     stock_price[\"close\"],stock_price[\"volume\"], window=14)\n",
    "VWAP = indicator.volume_weighted_average_price()\n",
    "\n",
    "indicator = ta.trend.SMAIndicator(stock_price[\"close\"], window=7)\n",
    "SMA7 = indicator.sma_indicator()\n",
    "\n",
    "indicator = ta.trend.SMAIndicator(stock_price[\"close\"], window=14)\n",
    "SMA14 = indicator.sma_indicator()\n",
    "\n",
    "VMAP_df = pd.concat([stock_price[\"date\"], VWAP], axis=1)\n",
    "VMAP_df[\"name\"] = \"VWAP\"\n",
    "VMAP_df = VMAP_df.rename(columns={VMAP_df.columns[0]: \"date\", VMAP_df.columns[1]: \"value\"})\n",
    "SMA7_df = pd.concat([stock_price[\"date\"], SMA7], axis=1)\n",
    "SMA7_df[\"name\"] = \"SMA7\"\n",
    "SMA7_df = SMA7_df.rename(columns={SMA7_df.columns[1]: \"date\", SMA7_df.columns[1]: \"value\"})\n",
    "SMA14_df = pd.concat([stock_price[\"date\"], SMA14], axis=1)\n",
    "SMA14_df[\"name\"] = \"SMA14\"\n",
    "SMA14_df = SMA14_df.rename(columns={SMA14_df.columns[1]: \"date\", SMA14_df.columns[1]: \"value\"})\n",
    "VMAP_df = VMAP_df.append([SMA7_df, SMA14_df])\n",
    "\n",
    "fig = px.line(VMAP_df, x=\"date\", y=\"value\", color=\"name\")\n",
    "fig.update_layout(template=\"simple_white\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = pd.DataFrame()\n",
    "indicators[\"OBV\"] = OBV\n",
    "indicators[\"ADI\"] = ADI\n",
    "indicators[\"AI\"] = AI\n",
    "# indicators[\"AID\"] = AID\n",
    "# indicators[\"AIU\"] = AIU\n",
    "indicators[\"VWAP\"] = VWAP\n",
    "# indicators[\"SMA7\"] = SMA7\n",
    "# indicators[\"SMA14\"] = SMA14\n",
    "indicators[\"close\"] = stock_price[\"close\"]\n",
    "\n",
    "indicators = indicators.dropna()\n",
    "indicators.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "transformer = MinMaxScaler().fit(indicators)\n",
    "scaled_indicators = pd.DataFrame(transformer.transform(indicators), columns=indicators.columns)\n",
    "scaled_indicators.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 14\n",
    "predicted_values = [\"close\"]\n",
    "\n",
    "train_data_length = (0, round(0.7 * scaled_indicators.shape[0]))\n",
    "val_data_length = (round(0.7 * scaled_indicators.shape[0])+1, round(0.9 * scaled_indicators.shape[0]))\n",
    "test_data_length = (round(0.9 * scaled_indicators.shape[0])+1, scaled_indicators.shape[0])\n",
    "\n",
    "train_x = generate_input_sequences(data=scaled_indicators[train_data_length[0]:train_data_length[1]],\n",
    "                                   input_length=sequence_length)\n",
    "train_y = generate_output_sequences(data=scaled_indicators[train_data_length[0]:train_data_length[1]],\n",
    "                                    input_length=sequence_length, outputs=predicted_values)\n",
    "\n",
    "val_x = generate_input_sequences(data=scaled_indicators[val_data_length[0]:val_data_length[1]],\n",
    "                                 input_length=sequence_length)\n",
    "val_y = generate_output_sequences(data=scaled_indicators[val_data_length[0]:val_data_length[1]],\n",
    "                                  input_length=sequence_length, outputs=predicted_values)\n",
    "\n",
    "test_x = generate_input_sequences(data=scaled_indicators[test_data_length[0]:test_data_length[1]],\n",
    "                                  input_length=sequence_length)\n",
    "test_y = generate_output_sequences(data=scaled_indicators[test_data_length[0]:test_data_length[1]],\n",
    "                                   input_length=sequence_length, outputs=predicted_values)\n",
    "\n",
    "print(f\"Train_x: {train_x.shape}\")\n",
    "print(f\"Train_y: {train_y.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Val_x:   {val_x.shape}\")\n",
    "print(f\"Val_y:   {val_y.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Test_x:  {test_x.shape}\")\n",
    "print(f\"Test_y:  {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-circular",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import metrics\n",
    "\n",
    "model_name = \"First Model\"\n",
    "\n",
    "model = models.Sequential(name=model_name)\n",
    "\n",
    "model.add(layers.LSTM(units=50, activation=\"relu\", name=\"LSTM_1\", dropout=0.05, \n",
    "                      input_shape=(train_x.shape[1], train_x.shape[2]), \n",
    "                      return_sequences=True))\n",
    "\n",
    "model.add(layers.LSTM(units=100, activation=\"relu\", name=\"LSTM_2\", dropout=0.1))\n",
    "\n",
    "model.add(layers.Dense(units=200, activation=\"relu\", name=\"Dense_1\"))\n",
    "model.add(layers.Dropout(0.15, name=\"Dropout_1\"))\n",
    "\n",
    "model.add(layers.Dense(units=400, activation=\"relu\", name=\"Dense_2\"))\n",
    "model.add(layers.Dropout(0.25, name=\"Dropout_2\"))\n",
    "\n",
    "model.add(layers.Dense(units=50, activation=\"relu\", name=\"Dense_3\"))\n",
    "\n",
    "model.add(layers.Dense(1, activation=\"tanh\", name=\"classifier\"))\n",
    "\n",
    "model.compile(optimizer=\"Adam\", \n",
    "              loss=\"mae\", \n",
    "              metrics=[\"mean_absolute_percentage_error\"])\n",
    "model.summary()\n",
    "\n",
    "history =  model.fit(train_x, train_y,\n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size, \n",
    "                     validation_data=(val_x, val_y))\n",
    "\n",
    "\n",
    "model_save_name = next_free_model_name(model_name)\n",
    "model_save_path = MODELS_DIR + \"/\" + model_save_name + \".h5\"\n",
    "model.save(model_save_path)\n",
    "\n",
    "history_save_path = MODELS_DIR + \"/\" + model_save_name + \".csv\"\n",
    "history_dataframe = pd.DataFrame(history.history)\n",
    "history_dataframe.to_csv(history_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictions = model.predict(test_x)\n",
    "plt.rcParams[\"figure.figsize\"] = (16,6)\n",
    "scaler = MinMaxScaler().fit(indicators[\"close\"].to_numpy().reshape(-1,1))\n",
    "unscaled_predictions = scaler.inverse_transform(predictions)\n",
    "unscaled_test_data = scaler.inverse_transform(test_y)\n",
    "plt.plot(np.arange(len(predictions)), unscaled_predictions, label=\"Guess\")\n",
    "plt.plot(np.arange(len(predictions)), unscaled_test_data, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-theater",
   "metadata": {},
   "source": [
    "## Expanding dataset using multiple companies\n",
    "Data of all companies is stored in 3D tensor where 3rd dimension represents different company.  \n",
    "Data is scaled to range (-1,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "companies_to_process = 100\n",
    "\n",
    "company_data = []\n",
    "keys = prices.symbol.unique()\n",
    "for ticker in keys[:companies_to_process]:\n",
    "    stock_price = prices[prices[\"symbol\"] == ticker]\n",
    "    OBV = ta.volume.on_balance_volume(stock_price[\"close\"], stock_price[\"volume\"])\n",
    "    \n",
    "    indicator = ta.volume.AccDistIndexIndicator(stock_price[\"high\"], stock_price[\"low\"],\n",
    "                                            stock_price[\"close\"], stock_price[\"volume\"])\n",
    "    ADI = indicator.acc_dist_index()\n",
    "    \n",
    "    indicator = ta.trend.AroonIndicator(stock_price[\"close\"])\n",
    "    AI  = indicator.aroon_indicator()\n",
    "    \n",
    "    indicator = ta.momentum.RSIIndicator(close=stock_price[\"close\"], window=14)\n",
    "    RSI = indicator.rsi()\n",
    "    \n",
    "    indicator = ta.volume.VolumeWeightedAveragePrice(stock_price[\"high\"], stock_price[\"low\"], \n",
    "                                     stock_price[\"close\"],stock_price[\"volume\"], window=14)\n",
    "    VWAP = indicator.volume_weighted_average_price()\n",
    "    \n",
    "    new_indicators = pd.DataFrame()\n",
    "    new_indicators[\"OBV\"] = OBV\n",
    "    new_indicators[\"ADI\"] = ADI\n",
    "    new_indicators[\"AI\"] = AI\n",
    "    new_indicators[\"VWAP\"] = VWAP\n",
    "    new_indicators[\"close\"] = stock_price[\"close\"]\n",
    "    new_indicators = new_indicators.to_numpy()\n",
    "    \n",
    "    company_data.append(new_indicators)\n",
    "\n",
    "company_data = np.array(company_data, dtype=object)\n",
    "# max_price = max([np.max(company[:,-1]) for company in company_data])\n",
    "# price_data = scale_closing_data(company_data, max_price)\n",
    "price_data=company_data\n",
    "\n",
    "new_price_data = []\n",
    "for company in price_data:\n",
    "    new_comapny = company[~np.isnan(company).any(axis=1)]\n",
    "    new_price_data.append(new_comapny)\n",
    "\n",
    "price_data = np.array(new_price_data, dtype=object)\n",
    "scaled_price_data, scalers = scale_3d_price_data(price_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-atlas",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 14\n",
    "predicted_values = -1\n",
    "data_split = (0.7, 0.2, 0.1)\n",
    "\n",
    "# price_data = np.array([0,1,2,3,4,5,6,7,8,9,10,11])\n",
    "train_data_length = (0, round(data_split[0] * scaled_price_data.shape[0]))\n",
    "val_data_length = (train_data_length[1], train_data_length[1] + round(data_split[1] * scaled_price_data.shape[0]))\n",
    "test_data_length = (val_data_length[1], val_data_length[1] + round(data_split[2] * scaled_price_data.shape[0]))\n",
    "\n",
    "input_data, output_data = [], []\n",
    "for company in scaled_price_data:\n",
    "    input_data.append(generate_input_sequences(data=company, input_length=sequence_length))\n",
    "    output_data.append(generate_output_sequences(data=company, input_length=sequence_length, \n",
    "                                                 outputs=predicted_values))\n",
    "\n",
    "train_x_segmented = np.array(input_data[train_data_length[0]:train_data_length[1]], dtype=object)\n",
    "train_y_segmented = np.array(output_data[train_data_length[0]:train_data_length[1]], dtype=object)\n",
    "train_scalers = scalers[train_data_length[0]:train_data_length[1]]\n",
    "\n",
    "val_x_segmented = np.array(input_data[val_data_length[0]:val_data_length[1]], dtype=object)\n",
    "val_y_segmented = np.array(output_data[val_data_length[0]:val_data_length[1]], dtype=object)\n",
    "val_scalers = scalers[val_data_length[0]:val_data_length[1]]\n",
    "\n",
    "test_x_segmented = np.array(input_data[test_data_length[0]:test_data_length[1]], dtype=object)\n",
    "test_y_segmented = np.array(output_data[test_data_length[0]:test_data_length[1]], dtype=object)\n",
    "test_scalers = scalers[test_data_length[0]:test_data_length[1]]\n",
    "\n",
    "train_x = flatten_data(train_x_segmented)\n",
    "train_y = flatten_data(train_y_segmented)\n",
    "\n",
    "val_x = flatten_data(val_x_segmented)\n",
    "val_y = flatten_data(val_y_segmented)\n",
    "\n",
    "print(f\"Segmented train_x: {train_x_segmented.shape}\")\n",
    "print(f\"Segmented rain_y: {train_y_segmented.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Segmented val_x:   {val_x_segmented.shape}\")\n",
    "print(f\"Segmented val_y:   {val_y_segmented.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Segmented test_x:  {test_x_segmented.shape}\")\n",
    "print(f\"Segmented test_y:  {test_y_segmented.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Train_x:  {train_x.shape}\")\n",
    "print(f\"Train_y:  {train_y.shape}\")\n",
    "print(\"---------------------------------\")\n",
    "print(f\"Val_x:   {val_x.shape}\")\n",
    "print(f\"Val_y:   {val_y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-cream",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import metrics\n",
    "\n",
    "model_name = \"Second Model\"\n",
    "\n",
    "model = models.Sequential(name=model_name)\n",
    "\n",
    "model.add(layers.LSTM(units=50, activation=\"relu\", name=\"LSTM_1\", dropout=0.05, \n",
    "                      input_shape=(train_x.shape[1], train_x.shape[2]), \n",
    "                      return_sequences=True))\n",
    "\n",
    "model.add(layers.LSTM(units=100, activation=\"relu\", name=\"LSTM_2\", dropout=0.1))\n",
    "\n",
    "model.add(layers.Dense(units=200, activation=\"relu\", name=\"Dense_1\"))\n",
    "model.add(layers.Dropout(0.15, name=\"Dropout_1\"))\n",
    "\n",
    "model.add(layers.Dense(units=400, activation=\"relu\", name=\"Dense_2\"))\n",
    "model.add(layers.Dropout(0.25, name=\"Dropout_2\"))\n",
    "\n",
    "model.add(layers.Dense(units=50, activation=\"relu\", name=\"Dense_3\"))\n",
    "\n",
    "model.add(layers.Dense(1, activation=\"tanh\", name=\"classifier\"))\n",
    "\n",
    "model.compile(optimizer=\"Adam\", \n",
    "              loss=\"mae\", \n",
    "              metrics=[\"mean_absolute_percentage_error\"])\n",
    "model.summary()\n",
    "\n",
    "history =  model.fit(train_x, train_y,\n",
    "                     epochs=epochs,\n",
    "                     batch_size=batch_size, \n",
    "                     validation_data=(val_x, val_y))\n",
    "\n",
    "model_save_name = next_free_model_name(model_name)\n",
    "model_save_path = MODELS_DIR + \"/\" + model_save_name + \".h5\"\n",
    "model.save(model_save_path)\n",
    "\n",
    "history_save_path = MODELS_DIR + \"/\" + model_save_name + \".csv\"\n",
    "history_dataframe = pd.DataFrame(history.history)\n",
    "history_dataframe.to_csv(history_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model = keras.models.load_model(MODELS_DIR + \"/\" + \"Second Model_001.h5\")\n",
    "# model = keras.models.load_model(model_save_path)\n",
    "\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-peripheral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(test_y.shape)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "selected_company = 6\n",
    "test_x = test_x_segmented[selected_company]\n",
    "test_y = test_y_segmented[selected_company]\n",
    "scaler = test_scalers[selected_company]\n",
    "\n",
    "predictions = model.predict(test_x)\n",
    "inversed_predictions = scaler.inverse_transform(np.repeat(predictions, 5, axis=1))[:,-1]\n",
    "inversed_test = scaler.inverse_transform(np.repeat(test_y, 5, axis=1))[:,-1]\n",
    "# plt.plot(np.arange(len(inversed_predictions)), inversed_predictions)\n",
    "# plt.plot(np.arange(len(inversed_predictions)), inversed_test)\n",
    "fig = px.line(x=np.arange(len(inversed_predictions)), y=inversed_predictions)\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(inversed_predictions)), y=inversed_test, mode=\"lines\", \n",
    "                         showlegend=False, name=\"test_data\"))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(len(inversed_predictions)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
